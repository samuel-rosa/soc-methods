---
title: "Pedotransfer functions for prediction and harmonization of carbon and organic matter content data in southern Brazil: source code"
author: "Alessandro Samuel-Rosa"
date: "06 July 2018"
url: "https://docs.google.com/document/d/13CJdr6huHhpuwKMfz_u03m8dJE3_xLqhmS7lwc5Y5yE/edit?usp=sharing"
output: html_document
---

```{r}
# load packages
library(dplyr)
library(magrittr)
# library(grid)
library(latticeExtra)
library(glue)
```

# Material and Methods

## Soil data

The data can be downloaded from the Free Brazilian Repository for Open Soil Data. The dataset identification 
code is `ctb0029`. Here we use a copy of the data stored in folder `data`.

We create categorical variables using the clay content (`argila_naoh_esferas_pipeta`) and the organic matter
content determined at the commercial soil analysis laboratory of the Universidade Federal de Santa Maria
(`matorg_dicromato_30min80_eam`). For the latter, the classification is performed using the three classes used 
for fertilizer recommendations in Rio Grande do Sul and Santa Catarina. For convenience, we rename key soil
variables using short English names.

```{r, echo=FALSE, message=FALSE}
camada <- readr::read_csv(
  '../data/camada.csv', locale = readr::locale(decimal_mark = ','), comment = '#unidade', na = "-")
camada <-
  camada %>% 
  mutate(textura = ifelse(argila_naoh_esferas_pipeta <= 250, '0-250', '251-500')) %>% 
  mutate(textura = ifelse(argila_naoh_esferas_pipeta > 500, '501-1000', textura)) %>% 
  mutate(textura = as.factor(textura)) %>% 
  mutate(matorg = ifelse(matorg_dicromato_30min80_eam <= 25, '<25', '26-50')) %>%
  mutate(matorg = ifelse(matorg_dicromato_30min80_eam > 50, '>50', matorg)) %>%
  mutate(matorg = as.factor(matorg)) %>% 
  mutate(observacao_id = as.factor(observacao_id)) %>% 
  rename(clay = argila_naoh_esferas_pipeta) %>% 
  rename(toc = carbono_fornalha_1min950_cgdct) %>%
  rename(oc = carbono_dicromato_30min150_mohr) %>% 
  rename(tom = matorg_fornalha_120min360_massa) %>% 
  rename(om = matorg_dicromato_30min80_eam)
camada
```

Data on soil observations if processed. Land use info, `terra_usoatual`, is reclassified using FAO guidelines 
for soil description. The following codes are used:

* FS (Floresta): Semi-deciduous forest (vegetation slightly disturbed)
* U (Vegetação secundária): Not used and not managed, vegetation strongly disturbed by clearing, burning,
  ploughing (secondary vegetation, mix of semi-deciduous shrubs and tall grassland)
* AA (Agricultura): Annual field cropping
* FP (Silvicultura): Plantation forestry
* HE (Campo nativo): Animal husbandry (extensive grazing)

Soil taxa is recoded for plotting purposes using a lookup table downloaded from the Free Brazilian Repository
for Open Soil Data

```{r, echo=FALSE, message=FALSE}
sibcs <- readr::read_csv('../data/tabela-de-consulta.csv')
sibcs_recode <- as.list(sibcs$campo_codigo)
names(sibcs_recode) <- sibcs$campo_nome  
observacao <- readr::read_csv(
  '../data/observacao.csv', locale = readr::locale(decimal_mark = ','), comment = '#unidade', na = '-')
observacao <-
  observacao %>% 
  arrange(observacao_id) %>% 
  mutate(terra_usoatual = gsub('Floresta', 'FS', .data$terra_usoatual)) %>% 
  mutate(terra_usoatual = gsub('Vegetação secundária', 'U', .data$terra_usoatual)) %>%
  mutate(terra_usoatual = gsub('Agricultura', 'AA', .data$terra_usoatual)) %>%
  mutate(terra_usoatual = gsub('Silvicultura', 'FP', .data$terra_usoatual)) %>%
  mutate(terra_usoatual = gsub('Campo nativo', 'HE', .data$terra_usoatual)) %>% 
  mutate(taxon = recode(taxon_sibcs_2009, !!!sibcs_recode))
observacao
```

We create new categorical predictor variables using data on land use (forest, agriculture, grazing), sampling
depth (topsoil), and taxonomy (latossolo, argissolo, cambissolo, planossolo, neossolo). These are dummy 
variables composed of zeros and ones. Also, rows of the dataset not containing soil carbon / organic matter 
data are eliminated.

```{r}
camada <- camada[1:105, ]
camada <- 
  camada %>% 
  mutate(terra_usoatual = observacao$terra_usoatual) %>% 
  mutate(forest = ifelse(terra_usoatual == "FS", 1, 0)) %>% 
  mutate(agriculture = ifelse(terra_usoatual == "AA", 1, 0)) %>% 
  mutate(grazing = ifelse(terra_usoatual == "HE", 1, 0)) %>% 
  dplyr::select(-terra_usoatual) %>% 
  mutate(topsoil = ifelse(profund_inf <= 20, 1, 0)) %>% 
  mutate(taxon2 = observacao$taxon_sibcs_2009) %>% 
  mutate(taxon1 = sapply(taxon2, function (x) unlist(strsplit(x, " "))[1])) %>% 
  mutate(latossolo = ifelse(taxon1 == "Latossolo", 1, 0)) %>% 
  mutate(argissolo = ifelse(taxon1 == "Argissolo", 1, 0)) %>% 
  mutate(cambissolo = ifelse(taxon1 == "Cambissolo", 1, 0)) %>% 
  mutate(planossolo = ifelse(taxon1 == "Planossolo", 1, 0)) %>% 
  mutate(neossolo = ifelse(taxon1 == "Neossolo", 1, 0)) %>% 
  dplyr::select(-taxon1, -taxon2)
camada
```

Compute some summary statistics: number of taxonomic classes, and range of clay content.

```{r, eval = FALSE}
data.frame(
  taxa = nlevels(as.factor(observacao$taxon)),
  min_clay = min(camada$clay, na.rm = TRUE),
  max_clay = max(camada$clay, na.rm = TRUE)
  )
```

### TODO: Figure 1

Location of soil observations used in this study.

### Figure 2

Key characteristics of the soil samples, such as the clay content and sampling depth, and their sampling sites,
such as the soil classification and type of land use and occupation.

```{r, eval = FALSE}
p1 <- histogram(
  ~ clay, camada, xlab = 'Clay content (g/kg)', ylab = 'Percent of total', 
  col = 'lightgray',
  panel = function (...) {
    lattice::panel.grid(v = -1, h = -1)
    lattice::panel.histogram(...)
  },
  page = function (n) {
    grid.text(label = "A)", x = unit(0.04, "npc"), y = grid::unit(0.95, "npc"))
  }) +
  latticeExtra::layer(panel.abline(v = c(250, 500), lty = 'dotted'))
p2 <- histogram(
  ~ profund_sup, camada, xlab = 'Sampling depth (cm)', ylab = 'Percent of total', col = 'lightgray',
  panel = function (...) {
    lattice::panel.grid(v = -1, h = -1)
    lattice::panel.histogram(...)
  },
  page = function (n) {
    grid.text(label = "B)", x = unit(0.04, "npc"), y = grid::unit(0.95, "npc"))
  })
p3 <- barchart(
  observacao$taxon, xlab = 'Soil classification', ylab = 'Percent of total', horizontal = FALSE, 
  col = 'lightgray', scales = list(x = list(rot = 60)),
  panel = function (...) {
    lattice::panel.grid(h = -1, v = 0)
    lattice::panel.barchart(...)
  },
  page = function (n) {
    grid.text(label = "C)", x = unit(0.04, "npc"), y = grid::unit(0.95, "npc"))
  })
p4 <- barchart(
  observacao$terra_usoatual, xlab = 'Land use and ocupation', ylab = 'Percent of total', horizontal = FALSE, 
  col = 'lightgray',
  panel = function (...) {
    lattice::panel.grid(h = -1, v = 0)
    lattice::panel.barchart(...)
  },
  page = function (n) {
    grid.text(label = "D)", x = unit(0.04, "npc"), y = grid::unit(0.95, "npc"))
  })
png("../res/fig/features-soil-samples.png", width = 480 * 4, height = 480 * 4, res = 72 * 3)
gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
dev.off()
```

## Analytical methods

Includes a description of the four analytical methods used:

1. Total organic carbon (TOC) by dry combustion
2. Organic carbon (OC) by wet digestion
3. Organic matter (OM) by wet digestion
4. Total organic matter (TOM) by loss on ignition

### Figure 3

Empirical probability density of carbon (A and B) and organic matter (C and D) content in soil samples 
according to the four analytical methods and the theoretical normal probability density function (dashed line).

```{r, eval = FALSE}
l <- layer(
  panel.mathdensity(
    dmath = dnorm, args = list(mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE)), n = length(x), 
    col = 'black', lty = 'dashed'))
p1 <- histogram(
  ~ toc, camada, 
  xlab = expression('Total organic carbon (TOC), g kg'^'-1'), col = 'lightgray', type = "density", 
  panel = function (...) {
    panel.grid(v = -1, h = -1)
    panel.histogram(...)
  },
  page = function (n) {
    grid.text(label = "A)", x = unit(0.04, "npc"), y = grid::unit(0.95, "npc"))
  }) + l
p2 <- histogram(
  ~ oc, camada, 
  xlab = expression('Organic carbon (OC), g kg'^'-1'), col = 'lightgray', type = "density", 
  panel = function (...) {
    panel.grid(v = -1, h = -1)
    panel.histogram(...)
  },
  page = function (n) {
    grid.text(label = "B)", x = unit(0.04, "npc"), y = grid::unit(0.95, "npc"))
  }) + l
p3 <- histogram(
  ~ om, camada, 
  xlab = expression('Organic matter (OM), g dm'^'-3'), col = 'lightgray', type = "density", 
  panel = function (...) {
    panel.grid(v = -1, h = -1)
    panel.histogram(...)
  },
  page = function (n) {
    grid.text(label = "C)", x = unit(0.04, "npc"), y = grid::unit(0.95, "npc"))
  }) + l
p4 <- histogram(
  ~ tom, camada, 
  xlab = expression('Total organic matter (TOM), g kg'^'-1'), col = 'lightgray', type = "density", 
  panel = function (...) {
    panel.grid(v = -1, h = -1)
    panel.histogram(...)
  },
  page = function (n) {
    grid.text(label = "D)", x = unit(0.04, "npc"), y = grid::unit(0.95, "npc"))
  }) + l
png("../res/fig/carbon-histograms.png", width = 480 * 4, height = 480 * 4, res = 72 * 3)
gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
dev.off()
```

# Pedotransfer functions

## Model formulation

We test six model formulations, starting with predictor variables that are more readly available and ending 
with the most complex/expensive model. These are:

A. y ~ 0 + x
B. y ~ 1 + x
C. y ~ 1 + x + x^2
D. y ~ 1 + x + x^2 + topsoil
E. y ~ 1 + x + x^2 + topsoil + landuse
F. y ~ 1 + x + x^2 + topsoil + landuse + clay
G. y ~ 1 + x + x^2 + topsoil + landuse + clay + taxon

In models F and G, 'clay' is used to compose interaction terms with 'x' and 'x^2'.

```{r}
# Soil variables
soil_vars <- c("toc", "oc", "om", "tom")
soil_vars <- expand.grid(soil_vars, soil_vars, stringsAsFactors = FALSE)[, 2:1]
soil_vars <- soil_vars[!soil_vars[, 1] == soil_vars[, 2], ]

# Predictor variables
landuse <- "agriculture + forest + grazing"
taxon <- "argissolo + cambissolo + latossolo + neossolo + planossolo"

# Formulas
formulas <- lapply(1:nrow(soil_vars), function (i) {
  y <- soil_vars[i, 1]
  x <- soil_vars[i, 2]
  list(
    glue("{y} ~ 0 + {x}"), # A
    glue("{y} ~ {x}"), # B
    glue("{y} ~ {x} + I({x}^2)"), # C
    glue("{y} ~ {x} + I({x}^2) + topsoil"), # D
    glue("{y} ~ {x} + I({x}^2) + topsoil + {landuse}"), # E
    glue("{y} ~ {x}*clay + I({x}^2)*clay + topsoil + {landuse}"), # F
    glue("{y} ~ {x}*clay + I({x}^2)*clay + topsoil + {landuse} + {taxon}") # G
    )
})
names(formulas) <- sapply(formulas, function (x) x[[1]])
```

## Model estimation

We use weighted least squares (WLS) to estimate model parameters. Below, we test the implementation of WLS to
estimate the betas, make predictions and estimate prediction error variances. Estimates are compared with the
output of `lm` and `predict.lm`. This toy exercise is usefull to understand the estimation equations and their
differences compared to ordinary least squares.

```{r, eval=FALSE}
# Temporary data
tmp_data <- camada

# Test observations
i <- 96:105

# Estimate model using lm
fit <- lm(tom ~ om + clay, data = tmp_data[-i, ], weights = 1/om)
pred <- predict(fit, tmp_data[i, c("om", "clay")], se.fit = TRUE)

# Dependent variable
Y <- tmp_data[-i, "tom"] %>% as.matrix()

# Predictor variables
X <- data.frame(x0 = 1, tmp_data[-i, c("om", "clay")]) %>% as.matrix()

# Weights
W <- matrix(0, ncol = nrow(X), nrow = nrow(X))
diag(W) <- 1 / X[, "om"]

# Estimate betas: (X'WX)⁻¹X'WY
b_hat <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% Y %>% drop()
data.frame(b_hat = b_hat, lm = fit$coefficients)

# Residual sum of squares
rss <- sum(diag(W) * (Y - X %*% drop(b_hat))^2) / (nrow(X) - ncol(X))
data.frame(rss, lm = summary(fit)$sigma^2)

# Prediction at a point
x0 <- data.frame(x0 = 1, tmp_data[i, c("om", "clay")]) %>% as.matrix()
y_hat <- x0 %*% drop(b_hat)
data.frame(y_hat = y_hat, lm = pred$fit)

# Prediction error variance: RSS * (1 + x0 (X'WX)⁻¹x0')
pev <- rss * (1 + x0 %*% solve(t(X) %*% W %*% X) %*% t(x0)) %>% diag()
data.frame(pev = pev, lm = pred$se.fit^2 + pred$residual.scale^2)
```

## Model validation

Run leave-one-out cross-validation and compute performance measures:

* MedE: median error
* MedSE: median squared error
* MedAE: median absolute error
* MedSDR: median squared deviation ratio
* AVE: amount of variation explained (or model efficiency)
* mAVE: modified amount of variation explained (or modified model efficiency)

The median is used instead of the mean because it is less sensitive to outliers.

# Results

## Carbon and organic matter data

### Figure 4

Scatter plot matrix of the soil carbon and organic matter content measured using four different analytical 
methods and their relation to the total clay content and class (0-250, 251-500, 501-1000 g kg-1). The solid 
line represents a perfect 1:1 linear relation, while the dashed line is the observed empirical linear relation
between variables.

```{r, eval = FALSE}
outliers <- c(58, 74, 66, 69, 70, 101, 102, 103, 105)
png("../res/fig/scatter-plot-matrix.png", width = 480 * 4, height = 480 * 4, res = 72 * 3)
p1 <- 
  camada %>% 
  select(toc, oc, om, tom, clay) %>% 
  splom(groups = camada$textura, grid = TRUE, auto.key = list(columns = 3), xlab = '',
        varnames = c(expression(atop('TOC', 'g kg'^'-1')), expression(atop('OC', 'g kg'^'-1')),
                     expression(atop('OM', 'g dm'^'-3')), expression(atop('TOM', 'g kg'^'-1')),
                     expression(atop('Clay', 'g kg'^'-1')))) +
  latticeExtra::layer(panel.abline(a = 0, b = 1)) +
  latticeExtra::layer(panel.lmline(x = x, y = y, lty = 'dashed'))
# Potential outliers
# p1 <- p1 + latticeExtra::layer(panel.text(x = x[outliers], y = y[outliers], outliers, pos = 1, cex = 0.7))
p1
dev.off()
```

## Prediction performance

```{r}
cross_validation <- list()
# i <- 1
for (i in 1:length(formulas)) {
  
  forms <- formulas[[i]]
  y <- camada[[soil_vars[i, 1]]]
  resid <- mean(y) - y
  x <- camada[[soil_vars[i, 2]]]
  w <- 1 / x # weigths are inversely proportional to the predictor variable
  
  loocv <- list()
  # j <- 1
  for (j in 1:length(forms)) {
    f <- forms[[j]]
    
    out <- data.frame(pred = NA_real_, pev = NA_real_)
    # Leave-one-out cross-validation
    # k <- 1
    for (k in 1:nrow(camada)) {
      # Weighted least squares regression
      lm_fit <- lm(formula = f, data = camada[-k, ], weights = w[-k])
      pred <- predict.lm(object = lm_fit, newdata = camada[k, ], se.fit = TRUE)
      out[k, ] <- c(pred$fit, pred$se.fit^2 + pred$residual.scale^2)
    }
    
    # Cross-validation statistics
    error <- out$pred - y
    error_sqr <- error * error
    error_abs <- abs(error)
    loocv[[j]] <- data.frame(
      
      # Model
      f = f,
      p = LETTERS[j] %>% as.factor(),
      y = soil_vars[i, 1],
      x = soil_vars[i, 2],
      m = glue("{soil_vars[i, 1]} ~ {soil_vars[i, 2]}") %>% toupper() %>% as.factor(),
      
      # Median error measures
      MedE = median(error),
      MedAE = median(error_abs),
      MedSE = median(error_sqr),
      # MedSDR = 1 - qchisq(p = 0.5, df = 1) + median(error_sqr / out$pev),
      MedSDR = median(error_sqr / out$pev),
      
      # Model efficiency
      AVE = 1 - (sum(error_sqr) / sum(resid * resid)),
      mAVE = 1 - (sum(error_abs) / sum(abs(resid))) # less sensitive to outliers than AVE
    )
  }
  cross_validation[[i]] <- do.call(rbind, loocv)
}
cross_validation <- do.call(rbind, cross_validation)
```

### Figure 5

Leave-one-out cross-validation performance of PTFs estimated via WLS. Line color and type indicate the 
dependent and predictor variable, respectively. Measurement units of MedE, MedAE, and MedSE are g kg-1 -- for 
TOC, OC, and TOM -- and g dm-3 -- for OM, while MedSDR, AVE, and mAVE are unitless performance measures. Model
formulations are described in Table 2. Performance measures are defined in Table 3.

```{r, fig.asp=1}
col <- c("dodgerblue", "magenta", "olivedrab", "gold") %>% rep(each = 3)
lty <- c(1, 2, 3, 4, 2, 3, 4, 1, 3, 4, 1, 2)
p1 <-
  xyplot(
    AVE + mAVE + MedAE + MedSE + MedE + MedSDR ~ p, data = cross_validation, groups = m,
    type = "l", col = col, lty = lty, lwd = 3, xlab = "Model formulation", ylab = "",
    key = list(lines = list(col = col, lty = lty, lwd = 3), columns = 4, 
               text = list(levels(cross_validation$m))),
    scales = list(x = list(relation = "same"), y = list(relation = "free")), layout = c(2, 3),
    panel = function (...) {
      panel.grid(h = -1, v = -1)
      panel.xyplot(...)
    })
png("../res/fig/cross-validation.png", width = 480 * 4, height = 480 * 5, res = 72 * 3)
p1
dev.off()
```




































We define the formulas for the linear models that will be tested.

```{r, echo=FALSE, eval=FALSE}
# Define formulas
soil_vars <- c("toc", "oc", "om", "tom")
soil_vars <- expand.grid(soil_vars, soil_vars, stringsAsFactors = FALSE)[, 2:1]
soil_vars <- soil_vars[!soil_vars[, 1] == soil_vars[, 2], ]

# One predictor
forms1 <- data.frame(y = soil_vars[, 1], x1 = soil_vars[, 2])
forms1 <- apply(forms1, 1, function (x) reformulate(x[2:ncol(forms1)], x[1]))

# Two predictors
forms2 <- data.frame(y = soil_vars[, 1], x1 = soil_vars[, 2], x2 = glue("I({soil_vars[, 2]}^2)"))
forms2 <- apply(forms2, 1, function (x) reformulate(x[2:ncol(forms2)], x[1]))

# Three predictors
forms3 <- data.frame(y = soil_vars[, 1], x1 = soil_vars[, 2], x2 = glue("I({soil_vars[, 2]}^2)"), x3 = "clay")
forms3 <- apply(forms3, 1, function (x) reformulate(x[2:ncol(forms3)], x[1]))

# Use log-transformed dependent variable
# forms1 <- lapply(forms1, function (x) update.formula(x, log(.) ~ .))
# forms2 <- lapply(forms2, function (x) update.formula(x, log(.) ~ .))
# forms3 <- lapply(forms3, function (x) update.formula(x, log(.) ~ .))

# names
names(forms1) <- sapply(forms1, function (x) x)
names(forms2) <- sapply(forms2, function (x) x)
names(forms3) <- sapply(forms3, function (x) x)
```

Run leave-one-out cross-validation and compute performance measures:

* MedE: median error
* MedSE: median squared error
* MedAE: median absolute error
* MedSDR: median squared deviation ratio
* AVE: amount of variation explained (or model efficiency)
* mAVE: modified amount of variation explained (or modified model efficiency)

The median is used instead of the mean because it is less sensitive to outliers.

```{r, eval=FALSE}
all_forms <- list(forms1, forms2, forms3)
dataset <- camada[1:105, ]
full_cv <- list()
# k <- 1
for (k in 1:length(all_forms)) {
  forms <- all_forms[[k]]
  cv <- list()
  # j <- 1
  for (j in 1:length(forms)) {
    f <- forms[[j]]
    y <- dataset[[soil_vars[j, 1]]]
    x <- dataset[[soil_vars[j, 2]]]
    w <- 1 / x # weigths are inversely proportional to the predictor variable
    out <- data.frame(pred = NA_real_, pev = NA_real_)
    
    # Leave-one-out cross-validation
    # i <- 1
    for (i in 1:nrow(dataset)) {
      # Weighted least squares regression
      lm_fit <- lm(formula = f, data = dataset[-i, ], weights = w[-i])
      pred <- predict.lm(object = lm_fit, newdata = dataset[i, ], se.fit = TRUE)
      out[i, ] <- c(pred$fit, pred$se.fit^2 + pred$residual.scale^2)
      
      # When using log-transformed dependent variable
      # lm_fit <- lm(formula = f, data = dataset[-i, ])
      # pred <- predict.lm(object = lm_fit, newdata = dataset[i, ], se.fit = TRUE)
      # pred <- data.frame(pred = pred$fit, pev = pred$se.fit^2 + pred$residual.scale^2)
      # out[i, "pred"] <- exp(pred$pred + 0.5 * pred$pev)
      # out[i, "pev"] <- (exp(pred$pev) - 1) * exp(2 * pred$pred + pred$pev)
    }
    
    # Cross-validation statistics
    resid <- mean(y) - y
    error <- out$pred - y
    error_sqr <- error * error
    error_abs <- abs(error)
    cv[[j]] <- data.frame(
      
      # Mean error measures are sensitive to outliers
      # ME = mean(error),
      # MAE = mean(error_abs),
      # MSE = mean(error_sqr),
      # MSDR = mean(out$pev / error_sqr),
      
      # Median error measures
      MedE = median(error),
      MedAE = median(error_abs),
      MedSE = median(error_sqr),
      MedSDR = qchisq(0.5, 1) + median(out$pev / error_sqr),
      
      # Model efficiency
      AVE = 1 - (sum(error_sqr) / sum(resid * resid)),
      mAVE = 1 - (sum(error_abs) / sum(abs(resid))) # less sensitive to outliers than AVE
    )
  }
  names(cv) <- names(forms)
  cv <- do.call(rbind, cv) %>% round(3)
  full_cv[[k]] <- cv
  # write.csv(cv, file = glue("../res/tab/cv-{k}-predictor.csv"))
}
names(full_cv) <- 1:3
full_cv <- do.call(rbind, full_cv)
```

```{r}
full_cv$p <- rep(c("y ~ x0 + x1", "y ~ x0 + x1 + x2", "y ~ x0 + x1 + x2 + x3"), each = 12) %>% as.factor()
full_cv$y <- rep(soil_vars[, 1], 3) %>% as.factor()
full_cv$x <- rep(soil_vars[, 2], 3) %>% as.factor()
full_cv$id <- glue("{full_cv$y} ~ {full_cv$x}") %>% toupper() %>% as.factor()
```

```{r, fig.asp=1}
col <- c("dodgerblue", "magenta", "olivedrab", "gold") %>% rep(each = 3)
lty <- c(1, 2, 3, 4, 2, 3, 4, 1, 3, 4, 1, 2)
p1 <-
  xyplot(
    AVE + mAVE + MedAE + MedSE + MedE + MedSDR ~ p, data = full_cv, groups = id,
    type = "l", col = col, lwd = 3, lty = lty, xlab = "Model formulation", ylab = "",
    key = list(lines = list(col = col, lty = lty, lwd = 3), columns = 4, text = list(levels(full_cv$id))),
    scales = list(x = list(relation = "same"), y = list(relation = "free")), layout = c(2, 3),
    panel = function (...) {
      panel.grid(h = -1, v = 0)
      panel.xyplot(...)
    })
# lim <- apply(full_cv[c("MRSE", "MedSDR", "MAE", "MedAE", "MSE", "MedSE")], 2, range)
# lim <- sapply(list(c(1:2), c(3:4), c(5:6)), function (i) extendrange(lim[, i])) %>% as.data.frame()
# lim <- lim[, rep(1:3, each = 2)] %>% as.list()
# p1$y.limits <- lim
png("../res/fig/cross-validation-wls-out.png", width = 480 * 4, height = 480 * 5, res = 72 * 3)
p1
dev.off()
```

An alternative to weighted least squares is quantile regression. With quantile regression we make predicitons 
at many points of the probability distribution of the dependent variable and then compute the mean and variance
of those estimates. We first evaluate the effect of the number of quantiles on the stability of the estimated
variance. Apparently, irrespective of the dependent and predictor variable, the variance starts to stabilize 
with 500 quantiles.

```{r, eval=FALSE, fig.asp=1}
length.out <- c(10, 50, 100, 500, 1000, 10000)
n <- list()
for (j in 1:length(length.out)) {
  pred <- data.frame(id = NA_integer_, pred = NA_real_, pev = NA_real_)
  for (i in 1:105) {
    fit <- rq(oc ~ toc + I(toc^2) + clay, tau = seq(0.01, 0.99, length.out = length.out[j]), data = dataset[-i, ])
    out <- predict(object = fit, newdata = dataset[i, ]) %>% as.numeric()
    pred[i, ] <- c(length.out[j], mean(out), var(out))
  }
  n[[j]] <- pred
}
n <- do.call(rbind, n)
n$id <- as.factor(n$id)
bwplot(pev ~ id, n) + layer(panel.grid(h = -1, v = 0))
```

```{r}
all_forms <- list(forms1, forms2, forms3)
dataset <- camada[1:105, ]
full_cv <- list()

# Loop over model formulations (p = 1, 2, 3)
# k <- 1
for (k in 1:length(all_forms)) {
  forms <- all_forms[[k]]
  cv <- list()
  
  # Loop over models (variation of y and x)
  # j <- 1
  for (j in 1:length(forms)) {
    f <- forms[[j]]
    y <- dataset[[soil_vars[j, 1]]]
    x <- dataset[[soil_vars[j, 2]]]
    out <- data.frame(pred = NA_real_, pev = NA_real_)
    
    # Leave-one-out cross-validation
    # i <- 1
    for (i in 1:nrow(dataset)) {
      
      # Quantile regression
      tau <- c(0.05, 0.95, seq(0.30, 0.70, length.out = 500))
      rq_fit <- rq(formula = f, data = dataset[-i, ], tau = tau)
      pred <- predict.rq(object = rq_fit, newdata = dataset[i, ]) %>% as.numeric()
      out[i, ] <- c(mean(pred), var(pred))
    }
    
    # Cross-validation statistics
    resid <- mean(y) - y
    error <- out$pred - y
    error_sqr <- error * error
    error_abs <- abs(error)
    cv[[j]] <- data.frame(
      
      # Mean error measures are sensitive to outliers
      # ME = mean(error),
      # MAE = mean(error_abs),
      # MSE = mean(error_sqr),
      # MSDR = mean(out$pev / error_sqr),
      
      # Median error measures
      MedE = median(error),
      MedAE = median(error_abs),
      MedSE = median(error_sqr),
      MedSDR = qchisq(0.5, 1) + median(out$pev / error_sqr),
      
      # Model efficiency
      AVE = 1 - (sum(error_sqr) / sum(resid * resid)),
      mAVE = 1 - (sum(error_abs) / sum(abs(resid))) # less sensitive to outliers than AVE
    )
  }
  names(cv) <- names(forms)
  cv <- do.call(rbind, cv) %>% round(3)
  full_cv[[k]] <- cv
}
names(full_cv) <- 1:3
full_cv <- do.call(rbind, full_cv)
```

```{r}
full_cv$p <- rep(c("y ~ x0 + x1", "y ~ x0 + x1 + x2", "y ~ x0 + x1 + x2 + x3"), each = 12) %>% as.factor()
full_cv$y <- rep(soil_vars[, 1], 3) %>% as.factor()
full_cv$x <- rep(soil_vars[, 2], 3) %>% as.factor()
full_cv$id <- glue("{full_cv$y} ~ {full_cv$x}") %>% toupper() %>% as.factor()
```

```{r, fig.asp=1}
col <- c("dodgerblue", "magenta", "olivedrab", "gold") %>% rep(each = 3)
lty <- c(1, 2, 3, 4, 2, 3, 4, 1, 3, 4, 1, 2)
p1 <-
  xyplot(
    AVE + mAVE + MedAE + MedSE + MedE + MedSDR ~ p, data = full_cv, groups = id,
    type = "l", col = col, lwd = 3, lty = lty, xlab = "Model formulation", ylab = "",
    key = list(lines = list(col = col, lty = lty, lwd = 3), columns = 4, text = list(levels(full_cv$id))),
    scales = list(x = list(relation = "same"), y = list(relation = "free")), layout = c(2, 3),
    panel = function (...) {
      panel.grid(h = -1, v = 0)
      panel.xyplot(...)
    })
# lim <- apply(full_cv[c("MRSE", "MedSDR", "MAE", "MedAE", "MSE", "MedSE")], 2, range)
# lim <- sapply(list(c(1:2), c(3:4), c(5:6)), function (i) extendrange(lim[, i])) %>% as.data.frame()
# lim <- lim[, rep(1:3, each = 2)] %>% as.list()
# p1$y.limits <- lim
png("../res/fig/cross-validation-qr.png", width = 480 * 4, height = 480 * 5, res = 72 * 3)
p1
dev.off()
```







```{r, fig.asp=1}
col <- c("dodgerblue", "magenta", "olivedrab", "gold") %>% rep(each = 3)
lty <- c(1, 2, 3, 4, 2, 3, 4, 1, 3, 4, 1, 2)
p1 <-
xyplot(
  mAVE + AVE + MSE + MAE + ME ~ as.factor(p), full_cv, 
  groups = id,  type = "l", 
  # col = col, lty = lty, 
  lwd = 3,
  xlab = "Model formulation", scales = list(x = list(relation = "same"), y = list(relation = "sliced")),
  # key = list(lines = list(col = col, lty = lty, lwd = 3), columns = 4, text = list(levels(full_cv$id2))),
  panel = function (...) {
    panel.grid(h = -1, v = 0)
    panel.xyplot(...)
  })
p1 <- 
  latticeExtra::useOuterStrips(
    p1, strip = lattice::strip.custom(bg = "bisque"), strip.left = lattice::strip.custom(bg = "lightblue"))
p1$aspect.ratio <- 1
lim <- 
  apply(full_cv[1:5], 2, function (x) by(x, full_cv$m, range)) %>% 
  sapply(function (x) c(x) %>% extendrange)
p1$y.limits <- lim[, rep(ncol(lim):1, each = 2)] %>% as.data.frame() %>% as.list()
png("../res/fig/cross-validation.png", width = 480 * 4, height = 480 * 6, res = 72 * 3)
p1
dev.off()
```


```{r}
all_forms <- list(forms1, forms2, forms3)
dataset <- camada[1:105, ]
full_cv <- list()
for (k in 1:length(all_forms)) {
  forms <- all_forms[[k]]
  cv <- list()
  for (j in 1:length(forms)) {
    f <- forms[[j]]
    y <- dataset[[soil_vars[j, 1]]]
    x <- dataset[[soil_vars[j, 2]]]
    w <- 1 / x
    err <- data.frame(rq = NA_real_, lm = NA_real_)
    # Leave-one-out cross-validation
    for (i in 1:nrow(dataset)) {
      # quantile regression
      rq_fit <- rq(f, data = dataset[-i, ], method = "fn")
      err[i, "rq"] <- predict(rq_fit, dataset[i, ]) - y[i]
      # weighted least squares regression
      lm_fit <- lm(f, data = dataset[-i, ], weights = w[-i])
      err[i, "lm"] <- predict.lm(lm_fit, dataset[i, ]) - y[i]
    }
    # Error statistics
    resid <- mean(y) - y
    err_sqr <- err^2
    err_abs <- abs(err)
    pev <- 
    cv[[j]] <- data.frame(
      ME = colMeans(err),
      MAE = colMeans(err_abs),
      MSE = colMeans(err_sqr),
      
      AVE = 1 - (colSums(err_sqr) / sum(resid^2)),
      mAVE = 1 - (colSums(err_abs) / sum(abs(resid)))
    )
  }
  names(cv) <- names(forms)
  cv <- do.call(rbind, cv) %>% round(3)
  full_cv[[k]] <- cv
  write.csv(cv, file = glue("../res/tab/cv-{k}-predictor.csv"))
}
names(full_cv) <- 1:3
full_cv <- do.call(rbind, full_cv)
```

```{r}
full_cv$p <- rep(c("y ~ x0 + x1", "y ~ x0 + x1 + x2", "y ~ x0 + x1 + x2 + x3"), each = 24) %>% as.factor()
full_cv$m <- rep(c("Quantile (median) regression", "Weighted least squares regression"), nrow(full_cv) / 2)
full_cv$y <- rep(soil_vars[, 1], each = ncol(err)) %>% rep(3) %>% as.factor()
full_cv$x <- rep(soil_vars[, 2], each = ncol(err)) %>% rep(3) %>% as.factor()
full_cv$id <- glue("{full_cv$m}({full_cv$y} ~ {full_cv$x})") %>% toupper() %>% as.factor()
full_cv$id2 <- glue("{full_cv$y} ~ {full_cv$x}") %>% toupper() %>% as.factor()
```

```{r, fig.asp=1}
col <- c("dodgerblue", "magenta", "olivedrab", "gold") %>% rep(each = 3)
lty <- c(1, 2, 3, 4, 2, 3, 4, 1, 3, 4, 1, 2)
p1 <-
xyplot(
  mAVE + AVE + MSE + MAE + ME ~ as.factor(p) | m, full_cv, 
  groups = id2,  type = "l", col = col, lty = lty, lwd = 3,
  xlab = "Model formulation", scales = list(x = list(relation = "same"), y = list(relation = "sliced")),
  key = list(lines = list(col = col, lty = lty, lwd = 3), columns = 4, text = list(levels(full_cv$id2))),
  panel = function (...) {
    panel.grid(h = -1, v = 0)
    panel.xyplot(...)
  })
p1 <- 
  latticeExtra::useOuterStrips(
    p1, strip = lattice::strip.custom(bg = "bisque"), strip.left = lattice::strip.custom(bg = "lightblue"))
p1$aspect.ratio <- 1
lim <- 
  apply(full_cv[1:5], 2, function (x) by(x, full_cv$m, range)) %>% 
  sapply(function (x) c(x) %>% extendrange)
p1$y.limits <- lim[, rep(ncol(lim):1, each = 2)] %>% as.data.frame() %>% as.list()
png("../res/fig/cross-validation.png", width = 480 * 4, height = 480 * 6, res = 72 * 3)
p1
dev.off()
```


```{r, eval=FALSE}
densidade <- 
  read.table("../data/densidade.csv", sep = "\t", dec = ",", head = TRUE) %>% 
  reshape(direction = "long", idvar = "id", varying = list(4:6), v.names = "d") %>% 
  mutate(id = as.factor(id), clay = as.numeric(clay))
fit1 <- nlme::lme(d ~ som + clay, data = densidade, random = ~1|id)
summary(fit)
plot(fit)
nlme::intervals(fit)
cbind(fit2$fitted, obs = densidade$d, fit = fitted(fit2)) %>% round(3)

fit2 <- lm(d ~ som + clay, data = densidade)
```

```{r, fig.asp=1, eval=FALSE}
mo <- log1p(round((camada$mo_wet * camada$densidade) / 10, 1))
clay <- camada$argila_naoh_pipeta
dens <- camada$densidade
plot(dens ~ clay)
fit <- lm(dens ~ mo*clay)
summary(fit)
```

```{r, eval=FALSE}
plot(fit, 1)
```


```{r, fig.asp=1, eval=FALSE}
plot(fit$fitted.values ~ dens)
abline(lm(fit$fitted.values ~ dens), col = "red")
abline(a = 0, b = 1)
```



```{r calibrate_models, echo=FALSE, eval=FALSE}
# Function to calibrate models for different values of tau
calibrate_models <-
  function (tau) {
    fit <- list()
    for (i in 1:length(forms)) {
      fit[[i]] <- rq(forms[[i]], tau = tau, data = camada, method = 'fn')
    }
    names(fit) <- names(forms)
    fit
  }
```

```{r, echo=FALSE, eval=FALSE}
# Calibrate models for tau = {0.025, 0.5, 0.975}
fit_lower <- calibrate_models(0.025)
fit_median <- calibrate_models(0.5)
fit_upper <- calibrate_models(0.975)
```

```{r prepare_beta_table, echo=FALSE, eval=FALSE}
# Function to prepare table with estimated model parameters (betas)
prepare_beta_table <-
  function (fit) {
    betas <- lapply(fit, function (x) summary(x)$coefficients)
    betas <- do.call(rbind, betas) %>% round(4)
    betas <- data.frame(PTF = rep(names(forms), each = 2), Beta = rownames(betas), betas)
    betas$PTF <- glue('**{betas$PTF}**')
    betas$PTF[seq(2, length(betas$PTF), 2)] <- ''
    rownames(betas) <- NULL
    colnames(betas)[3:5] <- c('Estimate', 'Lower', 'Upper')
    colnames(betas) <- glue('**{colnames(betas)}**')
    betas
  }
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# Table with estimated model parameters por tau = 0.5
tau <- 0.5
betas_median <- prepare_beta_table(fit_median)
panderOptions('round', rep(4, ncol(betas_median)))
panderOptions('table.split.cells', rep(100 / ncol(betas_median), ncol(betas_median)))
pander(betas_median, caption = glue('(\\#tab:coefficients-median) Estimated coefficients of carbon and organic matter pedotransfer functions and their respective confidence interval. Coefficients are as follows: $\\Beta_0$ -- intercept, $\\Beta_1$ -- soil carbon and organic matter content.'))
```

```{r get_y, echo=FALSE, eval=FALSE}
# Function to extract the name of the dependent variable in a formula
get_y <- function (form) {
  terms(form, simplify = T)[[2]] %>% as.character()
}
```

```{r cross-validation, echo=FALSE, eval=FALSE}
# Cross-validate models
cv <- list()
for (j in 1:length(forms)) {
  err <- vector()
  for (i in 1:nrow(camada)) {
    y <- camada[[get_y(forms[[j]])]]
    resid <- mean(y) - y
    cv_fit <- rq(forms[[j]], data = camada[-i, ], method = 'fn')
    err[i] <- predict(cv_fit, camada[i, ]) - y[i]
  }
  cv[[j]] <- data.frame(
    ME = mean(err),
    MedE = median(err),
    MAE = mean(abs(err)),
    MedAE = median(abs(err)),
    MSE = mean(err * err), 
    MedSE = median(err * err), 
    AVE = 1 - (sum(err * err) / sum(resid * resid)))
}
names(cv) <- names(forms)
cv <- do.call(rbind, cv) %>% round(4)
```

```{r, echo=FALSE, eval=FALSE}
colnames(cv) <- glue('**{colnames(cv)}**')
panderOptions('round', rep(4, ncol(cv)))
panderOptions('table.split.cells', rep(100 / ncol(cv), ncol(cv)))
pander(cv, caption = '(\\#tab:cross-validation) Leave-one-out cross-validation results of soil carbon and organic matter pedotransfer functions. Error statistics are as follows: ME -- mean error, MedE -- median error, MAE -- mean absolute error, MedAE -- median absolute error, MSE -- mean square error, MedSE -- median square error, AVE -- amount of variance explained.')
```

The estimated parameters of the quantile regressions for the lower ($\tau = 0.025$) and upper ($\tau = 0.975$) 
bounds of the 90% prediction interval of the carbon and organic matter pedotransfer functions are shown in 
Tables \@ref{tab:coefficients-lower} and \@ref{tab:coefficients-upper}, respectively.

```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# Table with estimated model parameters por tau = 0.025
tau <- 0.025
betas_lower <- prepare_beta_table(fit_lower)
panderOptions('round', rep(4, ncol(betas_lower)))
panderOptions('table.split.cells', rep(100 / ncol(betas_lower), ncol(betas_lower)))
pander(betas_lower, caption = glue('(\\#tab:coefficients-lower) Estimated coefficients for computing the lower bound ($\\tau = {tau}$) of the prediction interval of carbon and organic matter pedotransfer functions. Coefficients are as follows: $\\Beta_0$ -- intercept, $\\Beta_1$ -- soil carbon and organic matter content.'))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# Table with estimated model parameters por tau = 0.975
tau <- 0.975
betas_upper <- prepare_beta_table(fit_upper)
panderOptions('round', rep(4, ncol(betas_upper)))
panderOptions('table.split.cells', rep(100 / ncol(betas_upper), ncol(betas_upper)))
pander(betas_upper, caption = glue('(\\#tab:coefficients-upper) Estimated coefficients for computing the upper bound ($\\tau = {tau}$) of the prediction interval of carbon and organic matter pedotransfer functions. Coefficients are as follows: $\\Beta_0$ -- intercept, $\\Beta_1$ -- soil carbon and organic matter content.'))
```

# References